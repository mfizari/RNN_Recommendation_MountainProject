{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d90383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from collections import Counter\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "#Tensorflow impocarts\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf7a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyper_params,vocab, cuDNN=False):\n",
    "    \n",
    "    vocab_size=vocab['vocab_size_route']\n",
    "    embed_size = hyper_params['embed_size_route'] #embedding size \n",
    "    rnn_units = hyper_params['rnn_units'] #number of neurons per layer\n",
    "    dropout_val = hyper_params['dropout_val']\n",
    "    rec_dropout_val = hyper_params['rec_dropout_val']\n",
    "\n",
    "    if cuDNN:\n",
    "        #Build model\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, mask_zero=False, #mask_zero tells the following layers\n",
    "                                  input_shape = [None]),                                        #and the loss function to ignore the \n",
    "            tf.compat.v1.keras.layers.CuDNNLSTM(units=rnn_units, return_sequences=True),\n",
    "            tf.compat.v1.keras.layers.CuDNNLSTM(units=rnn_units, return_sequences=True),\n",
    "            keras.layers.TimeDistributed(keras.layers.Dense(vocab_size,\n",
    "                                                           activation='softmax'))\n",
    "        ])\n",
    "        \n",
    "        #Compile\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "    else:\n",
    "        #Build model\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, mask_zero=False, #mask_zero tells the following layers\n",
    "                                  input_shape = [None]),                                        #and the loss function to ignore the \n",
    "            keras.layers.LSTM(rnn_units, return_sequences=True,                                 #padding\n",
    "                             dropout = dropout_val, recurrent_dropout=rec_dropout_val),\n",
    "            keras.layers.LSTM(rnn_units, return_sequences=True,\n",
    "                             dropout = dropout_val, recurrent_dropout=rec_dropout_val),\n",
    "            keras.layers.TimeDistributed(keras.layers.Dense(vocab_size,\n",
    "                                                           activation='softmax'))\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Compile\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbaf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_all(hyper_params, vocab):\n",
    "\n",
    "    #Hyperparameters\n",
    "    embed_size_route = hyper_params['embed_size_route'] #embedding size \n",
    "    embed_size_ndays = hyper_params['embed_size_ndays'] #embedding size \n",
    "    rnn_units = hyper_params['rnn_units'] #number of neurons per layer\n",
    "    dropout_val = hyper_params['dropout_val']\n",
    "    rec_dropout_val = hyper_params['rec_dropout_val']\n",
    "    n_hidden_layers = hyper_params['n_hidden_layers'] \n",
    "    max_sen_len = hyper_params['max_sen_len']\n",
    "\n",
    "    #Define input layers\n",
    "    inputs = {}\n",
    "    inputs['route_id'] = keras.Input(shape=[None])  #shape=[max_sen_len-1]\n",
    "    inputs['ndays'] = keras.Input(shape=[None])\n",
    "    inputs['rating_yds'] = keras.Input(shape=[None])\n",
    "    inputs['rating_vscale'] = keras.Input(shape=[None])\n",
    "    inputs['rating_misc'] = keras.Input(shape=[None])\n",
    "    inputs['routetype'] = keras.Input(shape=[None])\n",
    "    \n",
    "\n",
    "    #Define embedding layers\n",
    "    emd_route = keras.layers.Embedding(input_dim=vocab['vocab_size_route'],\n",
    "                                       output_dim=embed_size_route)(inputs['route_id'])\n",
    "    \n",
    "    emd_ndays = keras.layers.Embedding(input_dim=vocab['vocab_size_bins'],\n",
    "                                       output_dim=embed_size_ndays)(inputs['ndays'])\n",
    "    \n",
    "    emd_yds = keras.layers.Embedding(input_dim=vocab['vocab_size_yds'],\n",
    "                                       output_dim=embed_size_ndays)(inputs['rating_yds'])\n",
    "    \n",
    "    emd_v = keras.layers.Embedding(input_dim=vocab['vocab_size_vscale'],\n",
    "                                       output_dim=embed_size_ndays)(inputs['rating_vscale'])\n",
    "    \n",
    "    emd_misc = keras.layers.Embedding(input_dim=vocab['vocab_size_misc'],\n",
    "                                       output_dim=embed_size_route)(inputs['rating_misc'])\n",
    "    \n",
    "    emd_type = keras.layers.Embedding(input_dim=vocab['vocab_size_type'],\n",
    "                                       output_dim=embed_size_ndays)(inputs['routetype'])\n",
    "    \n",
    "    \n",
    "    \n",
    "  \n",
    "    #Concatenate embedding layers\n",
    "    emd_concat = tf.keras.layers.Concatenate()([emd_route,\n",
    "                                                emd_ndays,\n",
    "                                                emd_yds,\n",
    "                                                emd_v,\n",
    "                                                emd_type])\n",
    "\n",
    "    #Batch normalization to speed up training\n",
    "    emd_concat = keras.layers.BatchNormalization()(emd_concat)\n",
    "\n",
    "    #LSTM layers (start with 1 layer, optionally 2)\n",
    "\n",
    "    RNN = tf.compat.v1.keras.layers.CuDNNLSTM(units=rnn_units,\n",
    "                                              return_sequences=True)(emd_concat)\n",
    "    if n_hidden_layers == 2:\n",
    "        RNN = tf.compat.v1.keras.layers.CuDNNLSTM(units=rnn_units,\n",
    "                                                  return_sequences=True)(RNN)\n",
    "\n",
    "    #Batch norm again\n",
    "    RNN = keras.layers.BatchNormalization()(RNN)\n",
    "\n",
    "    #Last layer - probabilities\n",
    "    output = keras.layers.Dense(vocab_size, activation='softmax')(RNN) #make sure this is actually working ....                                            \n",
    "\n",
    "    #Model\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f92ffd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tokenizer, word_index, vocab_size:\n",
    "fp_tokenizer = 'tokenizer_cuDNN_230203-023518.pickle'\n",
    "fp_wordindex = 'wordindex_cuDNN_230203-023518.json'\n",
    "with open(fp_tokenizer, \"rb\") as fp:\n",
    "    tokenizer = pickle.load(fp)\n",
    "with open(fp_wordindex, \"rb\") as fp:\n",
    "    word_index = json.load(fp)\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "#Load datasets\n",
    "train_set_route = tf.data.Dataset.load('Data/train_set_route')\n",
    "val_set_route = tf.data.Dataset.load('Data/val_set_route')\n",
    "train_set_ndays= tf.data.Dataset.load('Data/train_set_ndays')\n",
    "val_set_ndays = tf.data.Dataset.load('Data/val_set_ndays')\n",
    "train_set_all = tf.data.Dataset.load('Data/train_set_all')\n",
    "val_set_all = tf.data.Dataset.load('Data/val_set_all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4b47a",
   "metadata": {},
   "source": [
    "### Training route only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model\n",
    "route_rec_rnn = build_model(hyper_params, vocab)\n",
    "\n",
    "s = 'route'\n",
    "\n",
    "#Define paths and if you're restarting training or not\n",
    "training_restart = False\n",
    "\n",
    "if training_restart:\n",
    "    #Load weights If training was interupted:\n",
    "    date = '' #input date!\n",
    "    checkpoint_path = \"training_routerec_rnn_{s}_{date}/cp.ckpt\".format(s=s, date=date)\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "else:\n",
    "    #Define path for callback info. and define callback\n",
    "    date = datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "    checkpoint_path = \"training_routerec_rnn_{}/cp.ckpt\".format(date)\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    \n",
    "#Callbacks\n",
    "csv_logger = CSVLogger(\"model_history_log_{}.csv\".format(date), append=True) #saves for every epoch\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1,\n",
    "                                                 period=20) #only saves every 20 epochs! Saves time. \n",
    "\n",
    "#Train model\n",
    "route_rec_rnn.fit(train_set_route,\n",
    "          epochs = hyper_params['epochs'],\n",
    "          validation_data = val_set_route,\n",
    "          callbacks = [csv_logger, cp_callback])\n",
    "\n",
    "#Save entire model in checkpoint folder when finished training \n",
    "# Saving the entire model\n",
    "model_json = model.to_json()\n",
    "with open('model_{s}_{date}.json'.format(date=date, s=s), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(\"model_weights_{s}_{date}.h5\".format(s=s, date=date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3693d5b",
   "metadata": {},
   "source": [
    "### Training route/ndays  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd21215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model\n",
    "route_rec_rnn = build_model(hyper_params, vocab)\n",
    "\n",
    "s = 'ndays'\n",
    "\n",
    "#Define paths and if you're restarting training or not\n",
    "training_restart = False\n",
    "\n",
    "if training_restart:\n",
    "    #Load weights If training was interupted:\n",
    "    date = '' #input date!\n",
    "    checkpoint_path = \"training_routerec_rnn_{s}_{date}/cp.ckpt\".format(s=s, date=date)\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "else:\n",
    "    #Define path for callback info. and define callback\n",
    "    date = datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "    checkpoint_path = \"training_routerec_rnn_{}/cp.ckpt\".format(date)\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    \n",
    "#Callbacks\n",
    "csv_logger = CSVLogger(\"model_history_log_{}.csv\".format(date), append=True) #saves for every epoch\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1,\n",
    "                                                 period=20) #only saves every 20 epochs! Saves time. \n",
    "\n",
    "#Train model\n",
    "route_rec_rnn.fit(train_set_route,\n",
    "          epochs = hyper_params['epochs'],\n",
    "          validation_data = val_set_route,\n",
    "          callbacks = [csv_logger, cp_callback])\n",
    "\n",
    "\n",
    "#Save entire model in checkpoint folder when finished training \n",
    "# Saving the entire model\n",
    "model_json = model.to_json()\n",
    "with open('model_{s}_{date}.json'.format(date=date, s=s), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(\"model_weights_{s}_{date}.h5\".format(s=s, date=date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5ed95",
   "metadata": {},
   "source": [
    "### Training all features model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a45aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model\n",
    "route_rec_rnn = build_model(hyper_params, vocab)\n",
    "\n",
    "s = 'all'\n",
    "\n",
    "#Define paths and if you're restarting training or not\n",
    "training_restart = False\n",
    "if training_restart:\n",
    "    #Load weights If training was interupted:\n",
    "    date = '' #input date!\n",
    "    checkpoint_path = \"training_routerec_rnn_{s}_{date}/cp.ckpt\".format(s=s, date=date)\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "else:\n",
    "    #Define path for callback info. and define callback\n",
    "    date = datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "    checkpoint_path = \"training_routerec_rnn_{}/cp.ckpt\".format(date)\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    \n",
    "#Callbacks\n",
    "csv_logger = CSVLogger(\"model_history_log_{}.csv\".format(date), append=True) #saves for every epoch\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1,\n",
    "                                                 period=20) #only saves every 20 epochs! Saves time. \n",
    "\n",
    "#Train model\n",
    "route_rec_rnn.fit(train_set_route,\n",
    "          epochs = hyper_params['epochs'],\n",
    "          validation_data = val_set_route,\n",
    "          callbacks = [csv_logger, cp_callback])\n",
    "\n",
    "    \n",
    "#Save entire model in checkpoint folder when finished training \n",
    "# Saving the entire model\n",
    "model_json = model.to_json()\n",
    "with open('model_{s}_{date}.json'.format(date=date, s=s), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(\"model_weights_{s}_{date}.h5\".format(s=s, date=date))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
