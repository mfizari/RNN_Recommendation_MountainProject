{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82497ca5",
   "metadata": {},
   "source": [
    "### Validation of different models\n",
    "\n",
    "Here, we're going to compare the performance of various models to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from collections import Counter\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "#Tensorflow impocarts\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e2154e",
   "metadata": {},
   "source": [
    "Function for loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d403a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(modelname, weightsname):\n",
    "    json_file = open('{}.json'.format(modelname), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights(\"{}.h5\".format(weightsname)) # load weights into new model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c3d3f7",
   "metadata": {},
   "source": [
    "Load the validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a3677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tokenizer, word_index, vocab_size:\n",
    "fp_tokenizer = 'tokenizer_cuDNN_230203-023518.pickle'\n",
    "fp_wordindex = 'wordindex_cuDNN_230203-023518.json'\n",
    "with open(fp_tokenizer, \"rb\") as fp:\n",
    "    tokenizer = pickle.load(fp)\n",
    "with open(fp_wordindex, \"rb\") as fp:\n",
    "    word_index = json.load(fp)\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "#Load datasets\n",
    "val_set_route = tf.data.Dataset.load('Data/val_set_route')\n",
    "val_set_ndays = tf.data.Dataset.load('Data/val_set_ndays')\n",
    "val_set_all = tf.data.Dataset.load('Data/val_set_all')\n",
    "\n",
    "val_ datasets = [val_set_route, val_set_ndays, val_set_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f756a",
   "metadata": {},
   "source": [
    "Load other variables for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02015fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the output_index\n",
    "with open('Data/output_index.json', \"rb\") as fp:\n",
    "    output_index = json.load(fp)\n",
    "\n",
    "#load the hyperparameters\n",
    "with open('Data/hyper_params.json', \"rb\") as fp:\n",
    "    hyper_params = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b506b91",
   "metadata": {},
   "source": [
    "#### Define functions for evaluation\n",
    "\n",
    "Our network is sequence to sequence, so to get a prediction for the last tick in a sequence, we need to predict the entire sequence and then take the last element. The dimmension of the last element will be `vocab_size_routes` and the values will be probabilities of each route, so we simply use `argmax` to get the max. probability route:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7f5588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation metrics for models\n",
    "def get_model_eval_metrics(model, test_set):\n",
    "    \n",
    "    #Get predicitons and true values\n",
    "    y_true = np.array([])\n",
    "    y_preds = np.array([])\n",
    "\n",
    "    for X,Y in test_set:\n",
    "        x=X\n",
    "        y=Y\n",
    "\n",
    "        #Get predictions\n",
    "        preds = model.predict(x)\n",
    "        preds = np.argmax(preds, axis=2)[:,-1] #argmaxes for every user in a batch\n",
    "\n",
    "        #Get true\n",
    "        trues = y.numpy()[:,-1] #true vals for every user in a batch\n",
    "\n",
    "        #Append\n",
    "        y_preds = np.append(y_preds, preds)\n",
    "        \n",
    "        \n",
    "        y_true = np.append(y_true, trues)\n",
    "\n",
    "    #Convert to integers\n",
    "    y_preds = y_preds.astype(int)\n",
    "    y_true = y_true.astype(int)\n",
    "    \n",
    "    report = classification_report(y_true, y_preds, output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()[['precision','recall']]\n",
    "\n",
    "    # #Get the metrics\n",
    "    accuracy = df_report.loc['accuracy'].precision\n",
    "    precision = df_report.loc['weighted avg'].precision\n",
    "    recall = df_report.loc['weighted avg'].recall \n",
    "    \n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca005f7",
   "metadata": {},
   "source": [
    "We also need evaluation metrics for the baseline data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for baseline eval metrics\n",
    "\n",
    "def convert_baseline_to_pred(df_baseline,tokenizer, col='target'):\n",
    "    y_pred = tokenizer.texts_to_sequences([str(x) for x in df_baseline[col].values])\n",
    "    y_pred = [item for sublist in y_pred for item in sublist]\n",
    "    y_pred =np.array(y_pred)\n",
    "    return y_pred\n",
    "\n",
    "def get_model_eval_metrics(df_baseline, test_set, tokenizer, col):\n",
    "    \n",
    "    #Get predicitons and true values\n",
    "    y_true = np.array([])\n",
    "    y_preds = np.array([])\n",
    "\n",
    "    for X,Y in test_set:\n",
    "        #Get true\n",
    "        trues = Y.numpy()[:,-1] #true vals for every user in a batch\n",
    "        #Append        \n",
    "        y_true = np.append(y_true, trues)\n",
    "\n",
    "    #Get baseline prediction\n",
    "    y_preds = convert_baseline_to_pred(df_baseline, tokenizer, col)\n",
    "\n",
    "    #Convert to integers\n",
    "    y_preds = y_preds.astype(int)\n",
    "    y_true = y_true.astype(int)\n",
    "    \n",
    "    \n",
    "    #metrics\n",
    "    report = classification_report(y_true, y_preds, output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()[['precision','recall']]\n",
    "\n",
    "    # #Get the metrics\n",
    "    accuracy = df_report.loc['accuracy'].precision\n",
    "    precision = df_report.loc['weighted avg'].precision\n",
    "    recall = df_report.loc['weighted avg'].recall \n",
    "    \n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b84cca0",
   "metadata": {},
   "source": [
    "### Calculate confusion matrix for model based on test data:\n",
    "\n",
    "Load the baseline data and prepare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e9214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = pd.read_json('baselines.json')\n",
    "df_baseline = df_baseline.iloc[ouput_index] #filter by index \n",
    "df_baseline = df_baseline.iloc[0:n_records//10] #take the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d0c5f",
   "metadata": {},
   "source": [
    "Load all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f646be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all models with weights\n",
    "routerecrnn_routes = load_trained_model('Data/route_rec_rnn_route_023518',\n",
    "                                        'Data/route_rec_rnn_weights_route_023518')\n",
    "routerecrnn_days = load_trained_model('Data/route_rec_rnn_ndays_174708',\n",
    "                                      'Data/route_rec_rnn_weights_ndays_174708')\n",
    "routerecrnn_all = load_trained_model('Data/route_rec_rnn_all_092653',\n",
    "                                     'Data/route_rec_rnn_weights_all_092653')\n",
    "\n",
    "models = [routerecrnn_routes, routerecrnn_days, routerecrnn_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2838d0",
   "metadata": {},
   "source": [
    "Calculate evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6a97ba4-b5b1-4337-88c8-5761bb2289ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 268ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 293ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 308ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Calculate accuracy, precision, and recall for each model on each dataset\n",
    "mat_models = np.zeros((3,3)) \n",
    "for k in range(3):\n",
    "    mat_models[k,:] = get_model_eval_metrics(models[k], datasets[k])\n",
    "    \n",
    "#Calculate accuracy, precision, and recall for each baseline on one dataset (same true values)\n",
    "mat_baselines = np.zeros((3,3)) \n",
    "colnames = ['second', 'popular', 'popular_similar']\n",
    "for k in range(3):\n",
    "    mat_baselines[k,:] = get_model_eval_metrics_baseline(df_baseline, datasets[0], tokenizer, col=colnames[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92000fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Route_Model</th>\n",
       "      <th>Ndays_Model</th>\n",
       "      <th>AllFeatures_Model</th>\n",
       "      <th>second</th>\n",
       "      <th>popular</th>\n",
       "      <th>popular_similar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.825784</td>\n",
       "      <td>0.944251</td>\n",
       "      <td>0.944251</td>\n",
       "      <td>0.045296</td>\n",
       "      <td>0.052265</td>\n",
       "      <td>0.066202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.827526</td>\n",
       "      <td>0.945993</td>\n",
       "      <td>0.947735</td>\n",
       "      <td>0.043554</td>\n",
       "      <td>0.041504</td>\n",
       "      <td>0.052846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.825784</td>\n",
       "      <td>0.944251</td>\n",
       "      <td>0.944251</td>\n",
       "      <td>0.045296</td>\n",
       "      <td>0.052265</td>\n",
       "      <td>0.066202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Route_Model  Ndays_Model  AllFeatures_Model    second   popular  \\\n",
       "Accuracy      0.825784     0.944251           0.944251  0.045296  0.052265   \n",
       "Precision     0.827526     0.945993           0.947735  0.043554  0.041504   \n",
       "Recall        0.825784     0.944251           0.944251  0.045296  0.052265   \n",
       "\n",
       "           popular_similar  \n",
       "Accuracy          0.066202  \n",
       "Precision         0.052846  \n",
       "Recall            0.066202  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Output eval metric table\n",
    "\n",
    "mat_df={}\n",
    "modelnames = ['Route_Model', 'Ndays_Model','AllFeatures_Model']\n",
    "for k in range(len(modelnames)):\n",
    "    mat_df[modelnames[k]] = (mat_models)[k,:]\n",
    "for k in range(len(colnames)):\n",
    "    mat_df[colnames[k]] = (mat_baselines)[k,:]   \n",
    "\n",
    "pd.DataFrame(mat_df, index=['Accuracy', 'Precision', 'Recall'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
