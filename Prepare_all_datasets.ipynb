{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460eee92",
   "metadata": {},
   "source": [
    "This notebook prepares tensorflow datasets for training as well as the outputs from data pre-processing and saves in the `Data` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750203a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from collections import Counter\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "#Tensorflow impocarts\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159d0af8",
   "metadata": {},
   "source": [
    "Define functions that prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fec6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_num_ticks(df_input, hyper_params, cuDNN=False):\n",
    "    \n",
    "    #This function filters dataframe rows by the number of ticks\n",
    "    \n",
    "    #Length params from hyper_params\n",
    "    max_sen_len = hyper_params['max_sen_len']\n",
    "    ntick_cutoff = hyper_params['ntick_cutoff']\n",
    "    \n",
    "    #Get rid of rows where count(,) < max_sen_len-1\n",
    "    df_input = df_input[df_input.route_id.str.count(',') >= max_sen_len-1]\n",
    "\n",
    "    #trruncation function\n",
    "    def trunc(x, max_sen_len): #func to truncate\n",
    "        return ' '.join(x.split(',')[-max_sen_len::])\n",
    "\n",
    "    #Truncate all string to be same length\n",
    "    for col in df_input.columns:\n",
    "        if col != 'user_id':\n",
    "            df_input[col] = df_input[col].apply(lambda x: trunc(x, max_sen_len))\n",
    "\n",
    "    #Reset the dataframe index\n",
    "    ouput_index = df_input.index\n",
    "    df_input.reset_index(inplace=True)\n",
    "    df_input.drop('index',axis=1,inplace=True)\n",
    "    \n",
    "    return df_input, ouput_index\n",
    "\n",
    "def fit_tokenizer(text_to_fit, num_vals):\n",
    "    \n",
    "    #Parameters for tokenization\n",
    "    oov_token = '<UNK>' #this is the token for out of vocab\n",
    "\n",
    "    #Tokenize dataset\n",
    "    tokenizer_input = Tokenizer(num_words=num_vals, oov_token=oov_token)\n",
    "    tokenizer_input.fit_on_texts(text_to_fit)\n",
    "\n",
    "    #Get the word index dictionary\n",
    "    word_index = tokenizer_input.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    \n",
    "    return tokenizer_input, word_index, vocab_size\n",
    "\n",
    "def generate_train_val_data_select(df_input, hyper_params, vocab_size, word_index, tokenizer, features, cuDNN=True):   \n",
    "    \n",
    "    \n",
    "    #Parameters from hyper_params\n",
    "    ntick_cutoff = hyper_params['ntick_cutoff']\n",
    "    batch_size = hyper_params['batch_size']\n",
    "    max_sen_len = hyper_params['max_sen_len']\n",
    "    # trainsize = hyper_params['test_cutoff']\n",
    "    nyears = hyper_params['nyears'] \n",
    "    binsize = hyper_params['binsize']\n",
    "\n",
    "\n",
    "    #Filter by number of ticks\n",
    "    df_input, ouput_index = filter_num_ticks(df_input, hyper_params, cuDNN=cuDNN)\n",
    "\n",
    "\n",
    "    #Now generate \"text files\" from dataframe...\n",
    "    text_route = [] #each of these is just a list of strings, where L = users\n",
    "    text_ndays = []\n",
    "    text_rating_yds = []\n",
    "    text_rating_vscale = []\n",
    "    text_rating_misc = []\n",
    "    text_routetype = []\n",
    "\n",
    "    for i in range(len(df_input.index)):\n",
    "        text_route.append(df_input.route_id[i])\n",
    "        text_ndays.append(df_input.ndays[i])\n",
    "        text_rating_yds.append(df_input.rating_yds[i])\n",
    "        text_rating_vscale.append(df_input.rating_vscale[i])\n",
    "        text_rating_misc.append(df_input.rating_misc[i])\n",
    "        text_routetype.append(df_input.routetype[i])\n",
    "\n",
    "\n",
    "    #Get total number of records \n",
    "    n_records = len(text_route)\n",
    "\n",
    "    #Tokenize on rating_yds\n",
    "    tokenizer_rating_yds, word_index_rating_yds, vocab_size_rating_yds = fit_tokenizer(text_rating_yds, 20)\n",
    "\n",
    "    #Tokenize on rating_vscale\n",
    "    tokenizer_rating_vscale, word_index_rating_vscale, vocab_size_rating_vscale = fit_tokenizer(text_rating_vscale, 20)\n",
    "\n",
    "    #Tokenize on rating_misc\n",
    "    tokenizer_rating_misc, word_index_rating_misc, vocab_size_rating_misc = fit_tokenizer(text_rating_misc, 800)\n",
    "\n",
    "    #Tokenize on routetype\n",
    "    tokenizer_routetype, word_index_routetype, vocab_size_routetype = fit_tokenizer(text_routetype, 120)\n",
    "\n",
    "    #Generate feature and target sentences (shifted)\n",
    "    feature_text_route = [\" \".join(sen.split(' ')[0:-1]) for sen in text_route]\n",
    "    feature_text_ndays = [\" \".join(sen.split(' ')[0:-1]) for sen in text_ndays]\n",
    "    feature_text_rating_yds = [\" \".join(sen.split(' ')[0:-1]) for sen in text_rating_yds]\n",
    "    feature_text_rating_vscale = [\" \".join(sen.split(' ')[0:-1]) for sen in text_rating_vscale]\n",
    "    feature_text_rating_misc = [\" \".join(sen.split(' ')[0:-1]) for sen in text_rating_misc]\n",
    "    feature_text_routetype = [\" \".join(sen.split(' ')[0:-1]) for sen in text_routetype]\n",
    "    target_text = [\" \".join(sen.split(' ')[1::]) for sen in text_route]\n",
    "\n",
    "    #Produce bins\n",
    "    bins = np.arange(0, nyears*365 + binsize, binsize, dtype=int) #produce bins\n",
    "    vocab_size_bins = len(bins) + 1\n",
    "\n",
    "    #Bin feature_text_ndays in sequences directly\n",
    "    feature_seq_ndays = [[np.digitize(y, bins, right=False) for y in [int(x) for x in z.split(' ')]] for z in feature_text_ndays]    \n",
    "\n",
    "    #Directly convert to matricies\n",
    "    feature_seq_route = np.array(tokenizer.texts_to_sequences(feature_text_route)) #array remove need to use padding\n",
    "    feature_seq_ndays = np.array(feature_seq_ndays)\n",
    "    feature_seq_rating_yds = np.array(tokenizer_rating_yds.texts_to_sequences(feature_text_rating_yds)) #array remove need to use padding\n",
    "    feature_seq_rating_vscale = np.array(tokenizer_rating_vscale.texts_to_sequences(feature_text_rating_vscale)) #array remove need to use padding\n",
    "#     feature_seq_rating_misc = np.array(tokenizer_rating_misc.texts_to_sequences(feature_text_rating_misc)) #array remove need to use padding\n",
    "    feature_seq_routetype = np.array(tokenizer_routetype.texts_to_sequences(feature_text_routetype)) #array remove need to use padding\n",
    "    target_seq = np.array(tokenizer.texts_to_sequences(target_text))\n",
    "\n",
    "    #Make feature dictionary\n",
    "    if features == 'route_id':\n",
    "        feature_dict = feature_seq_route\n",
    "    \n",
    "    if features == 'ndays':\n",
    "        feature_dict = {'route_id': feature_seq_route,\n",
    "                        'ndays': feature_seq_ndays}\n",
    "        \n",
    "    if features == 'all':\n",
    "        feature_dict = {'route_id': feature_seq_route,\n",
    "                        'ndays': feature_seq_ndays,\n",
    "                        'rating_yds': feature_seq_rating_yds,\n",
    "                        'rating_vscale': feature_seq_rating_vscale, #'rating_misc': feature_seq_rating_misc,\n",
    "                        'routetype': feature_seq_routetype}\n",
    "\n",
    "\n",
    "    # Make a dataset tensor for training and one for validation\n",
    "    dataset_size = feature_seq_route.shape[0] #number of records\n",
    "    train_size = dataset_size * 90 // 100\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((feature_dict, target_seq)) #all data, shuffled perfectly\n",
    "\n",
    "    # #Split into train and val\n",
    "    # train_set_output = dataset.take(train_size) \n",
    "    # val_set_output = dataset.skip(train_size)\n",
    "    \n",
    "    \n",
    "    vocab = {\n",
    "        'vocab_size_route': vocab_size,\n",
    "        'vocab_size_bins': vocab_size_bins,\n",
    "        'vocab_size_yds': 20,\n",
    "        'vocab_size_vscale': 20,\n",
    "        'vocab_size_misc': 800,\n",
    "        'vocab_size_type': 120\n",
    "    }\n",
    "    \n",
    "    tokenizer_dict = {\n",
    "        'tokenizer_route': tokenizer,\n",
    "        'tokenizer_rating_yds': tokenizer_rating_yds,\n",
    "        'tokenizer_rating_vscale': tokenizer_rating_vscale,\n",
    "        'tokenizer_rating_misc': tokenizer_rating_misc,\n",
    "        'tokenizer_routetype': tokenizer_routetype\n",
    "        \n",
    "    }\n",
    "\n",
    "    return dataset, vocab, word_index, tokenizer_dict, n_records, ouput_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3980e38d",
   "metadata": {},
   "source": [
    "Read raw data, load previously prepared tokenizer and hyper_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b4c7955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "df = pd.read_json('tickdata_format_allfeatures.json')\n",
    "\n",
    "#Load tokenizer, word_index, vocab_size:\n",
    "import pickle \n",
    "fp_tokenizer = 'tokenizer_cuDNN_230203-023518.pickle'\n",
    "fp_wordindex = 'wordindex_cuDNN_230203-023518.json'\n",
    "with open(fp_tokenizer, \"rb\") as fp:\n",
    "    tokenizer = pickle.load(fp)\n",
    "with open(fp_wordindex, \"rb\") as fp:\n",
    "    word_index = json.load(fp)\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "#Define hyper_params:\n",
    "hyper_params= {\n",
    "    'ntick_cutoff': 50,        #min number of ticks to use a profile (only for non-cuDNN)\n",
    "    'batch_size': 32,          #obvious\n",
    "    'max_sen_len': 50,         #max length of sentence (number of ticks to use for cuDNN)\n",
    "    'binsize': 30,             #size of bins for binning ndays in days\n",
    "    'nyears': 20,              #max number of years to look-back\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee411872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Doug\\AppData\\Local\\Temp\\ipykernel_6588\\757796400.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input[col] = df_input[col].apply(lambda x: trunc(x, max_sen_len))\n",
      "C:\\Users\\Doug\\AppData\\Local\\Temp\\ipykernel_6588\\757796400.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input.drop('index',axis=1,inplace=True)\n",
      "C:\\Users\\Doug\\AppData\\Local\\Temp\\ipykernel_6588\\757796400.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input[col] = df_input[col].apply(lambda x: trunc(x, max_sen_len))\n",
      "C:\\Users\\Doug\\AppData\\Local\\Temp\\ipykernel_6588\\757796400.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input.drop('index',axis=1,inplace=True)\n",
      "C:\\Users\\Doug\\AppData\\Local\\Temp\\ipykernel_6588\\757796400.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input[col] = df_input[col].apply(lambda x: trunc(x, max_sen_len))\n",
      "C:\\Users\\Doug\\AppData\\Local\\Temp\\ipykernel_6588\\757796400.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_input.drop('index',axis=1,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#Generate datasets\n",
    "dataset_routes, vocab, word_index, tokenizer_dict, n_records, ouput_index = generate_train_val_data_select(df,\n",
    "                                                                                     hyper_params,\n",
    "                                                                                     vocab_size,\n",
    "                                                                                     word_index,\n",
    "                                                                                     tokenizer,\n",
    "                                                                                     features='route_id',\n",
    "                                                                                     cuDNN=True)\n",
    "dataset_ndays, vocab, word_index, tokenizer_dict, n_records, ouput_index = generate_train_val_data_select(df,\n",
    "                                                                                     hyper_params,\n",
    "                                                                                     vocab_size,\n",
    "                                                                                     word_index,\n",
    "                                                                                     tokenizer,\n",
    "                                                                                     features='ndays',\n",
    "                                                                                     cuDNN=True)\n",
    "dataset_all, vocab, word_index, tokenizer_dict, n_records, ouput_index = generate_train_val_data_select(df,\n",
    "                                                                                     hyper_params,\n",
    "                                                                                     vocab_size,\n",
    "                                                                                     word_index,\n",
    "                                                                                     tokenizer,\n",
    "                                                                                     features='all',\n",
    "                                                                                     cuDNN=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c89f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train/val and batch/prefetch\n",
    "val_set_route = dataset_routes.take(n_records//10).batch(hyper_params['batch_size']).prefetch(1)\n",
    "val_set_nday = dataset_ndays.take(n_records//10).batch(hyper_params['batch_size']).prefetch(1)\n",
    "val_set_all = dataset_all.take(n_records//10).batch(hyper_params['batch_size']).prefetch(1)\n",
    "\n",
    "train_set_route = dataset_routes.skip(n_records//10).batch(hyper_params['batch_size']).prefetch(1)\n",
    "train_set_nday = dataset_ndays.skip(n_records//10).batch(hyper_params['batch_size']).prefetch(1)\n",
    "train_set_all = dataset_all.skip(n_records//10).batch(hyper_params['batch_size']).prefetch(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6e4a5",
   "metadata": {},
   "source": [
    "Save the tensor data and other outputs to files in the Data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c303909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save training/test sets and output index \n",
    "train_set_route.save('Data/train_set_route')\n",
    "val_set_route.save('Data/val_set_route')\n",
    "train_set_nday.save('Data/train_set_nday')\n",
    "val_set_nday.save('Data/val_set_nday')\n",
    "train_set_all.save('Data/train_set_all')\n",
    "val_set_all.save('Data/val_set_all')\n",
    "\n",
    "\n",
    "    \n",
    "#Save hyper_params as a separate file as well, and vocab, tokenizer_dict\n",
    "with open('Data/hyper_params.json', 'w') as fp:\n",
    "    json.dump(hyper_params, fp)\n",
    "    \n",
    "with open('Data/output_index.pickle', 'wb') as fp:\n",
    "    pickle.dump(ouput_index, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('Data/tokenizer_route.pickle', \"wb\") as fp:\n",
    "    pickle.dump(tokenizer, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('Data/word_index.json', \"w\") as fp:\n",
    "    json.dump(word_index, fp)\n",
    "        \n",
    "with open('Data/vocab.json', \"w\") as fp:\n",
    "    json.dump(vocab, fp)\n",
    "    \n",
    "with open('Data/tokenizer_dict.pickle', \"wb\") as fp:\n",
    "    pickle.dump(tokenizer_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
